
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Multicam</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <link rel="icon" type="image/png" href="../img/logo.png">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Temporal and Contextual Transformer for 
                <br>Multi-Camera Editing of TV Shows
                <br>
                <small>
                    ECCVW 2022
                </small>
            </h1>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <div style="margin-bottom: 0.7em; margin-top:0.2em" class="authors">
                    <a style="color:#000000;" href="https://anyirao.com/">Anyi Rao<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="">Xuekun Jiang<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="">Sichen Wang<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="">Yuwei Guo<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="">Zihao Liu<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <br>
                    <a style="color:#000000;" href="">Dai Bo<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="">Long Pang<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="">Xiaoyu Wu<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="http://dahua.me/">Dahua Lin<sup>2,4</sup></a>
                    <a style="color:#000000;" href="">Libiao Jin<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                </div>

                <div style="margin-bottom: 0.5em;" class="affiliations">
                    <a href="https://cs.stanford.edu/">Stanford University<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    
                    <a href="https://www.shlab.org.cn/">Shanghai Artificial Intelligence Laboratory<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <br>
                    <a href="https://en.cuc.edu.cn/">Communication University of China<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  
                    <a href="http://mmlab.ie.cuhk.edu.hk/">The Chinese University of Hong Kong<sup>4</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    </br>
                </div>

            </div>
        </div>

        <div style="margin-bottom: 0.7em;" class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2210.08737">
                            <image src="../img/paper.png" height="50px"><br>
                                <h5><strong>Paper</strong></h5>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="">
                            <image src="../img/github_pad.png" height="50px"><br>
                                <h5><strong>Code (coming)</strong></h5>
                            </a>
                        </li> -->
                        <li>
                            <a href="https://drive.google.com/drive/folders/1V-ZKbwJgUD5rv3lI0dzCSAspLjm1mMMb?usp=sharing">
                            <image src="../img/paperclip.png" height="50px"><br>
                                <h5><strong>Data</strong></h5>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/multicam.jpg" class="img-responsive" alt="overview" style="width:100%;margin-left:auto;margin-right:auto;"><br>
                <p class="text-justify">
                    The ability to choose an appropriate camera view among multiple cameras plays a vital role in TV shows delivery. 
                    But it is hard to figure out the statistical pattern and apply intelligent processing due to the lack of high-quality training data. 
                    To solve this issue, we first collect a novel benchmark on this setting with four diverse scenarios including concerts, 
                    sports games, gala shows, and contests, where each scenario contains 6 synchronized tracks recorded by different cameras. 
                    It contains 88-hour raw videos that contribute to the 14-hour edited videos. Based on this benchmark, 
                    we further propose a new approach temporal and contextual transformer that utilizes clues from historical shots and other views to make shot transition decisions and predict which view to be used. 
                    Extensive experiments show that our method outperforms existing methods on the proposed multi-camera editing benchmark.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    TVMCE Dataset
                </h3>
                <image src="img/data.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    <strong> Overview of TV shows MultiCamera Editing (TVMCE) Datset.</strong>
                    We reached out to film and TV production major colleges and follow their professional production
                    team to acquire data covering various scenarios including
                    concert, sports, contest and gala show that happen in universities and city theaters/stadiums. 
                    Our dataset holds a balanced coverage ratio among different scenario categories
                    with 39% in gala shows and 14% in sports. 
                    Most shots in our dataset have a time duration between 0 to 8 seconds and 
                    a few shots are long shots that last longer than 32 seconds.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>

@article{rao2022temporal,
    title={Temporal and Contextual Transformer for Multi-Camera Editing of TV Shows},
    author={Rao, Anyi and Jiang, Xuekun and Wang, Sichen and Guo, Yuwei and Liu, Zihao and Dai, Bo and Pang, Long and Wu, Xiaoyu and Lin, Dahua and Jin, Libiao},
    journal={arXiv preprint arXiv:2210.08737},
    year={2022}
}
</textarea>
                </div>
            </div>
        </div>

       
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>

	<script type="text/javascript">
        var slideIndex = 1;
        showSlides(slideIndex);

        // Next/previous controls
        function plusSlides(n) {
        showSlides(slideIndex += n);
        }

        // Thumbnail image controls
        function currentSlide(n) {
        showSlides(slideIndex = n);
        }

        function showSlides(n) {
        var i;
        var slides = document.getElementsByClassName("mySlides");
        var dots = document.getElementsByClassName("dot");
        if (n > slides.length) {slideIndex = 1}
        if (n < 1) {slideIndex = slides.length}
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(" active", "");
        }
        slides[slideIndex-1].style.display = "block";
        dots[slideIndex-1].className += " active";
        }
	</script>




</html>
